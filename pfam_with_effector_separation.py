# -*- coding: utf-8 -*-
"""pfam_with_effector_separation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Cst-Al0BblcuqcTX_CNiZ0hxm9Y79gcY
"""

!pip install biopython

import requests
import time
import pandas as pd
from Bio import SeqIO
import re

def separator(fasta_file):
    # Output files for different effector types
    cytoplasmic_file = "cytoplasmic_effectors.fasta"
    apoplastic_file = "apoplastic_effectors.fasta"
    dual_file = "dual_effectors.fasta"

    # Open output files for writing
    with open(cytoplasmic_file, "w") as cyto_out, open(apoplastic_file, "w") as apo_out, open(dual_file, "w") as dual_out:

        # Read the input FASTA file
        with open(fasta_file, "r") as f:
            header = ""
            sequence = ""

            for line in f:
                line = line.strip()

                if line.startswith(">"):  # Header line
                    if header and sequence:
                        # Determine effector type and write to respective file
                        write_to_file(header, sequence, cyto_out, apo_out, dual_out)

                    header = line  # Store new header
                    sequence = ""  # Reset sequence
                else:
                    sequence += line  # Append sequence lines

            # Write the last sequence to appropriate file
            if header and sequence:
                write_to_file(header, sequence, cyto_out, apo_out, dual_out)

    return [cytoplasmic_file, apoplastic_file, dual_file]

def write_to_file(header, sequence, cyto_out, apo_out, dual_out):
    # Determine effector type based on header
    is_cytoplasmic = re.search(r"Cytoplasmic effector probability: ([0-9\.]+)", header)
    is_apoplastic = re.search(r"Apoplastic effector probability: ([0-9\.]+)", header)

    cyto_prob = float(is_cytoplasmic.group(1)) if is_cytoplasmic else 0.0
    apo_prob = float(is_apoplastic.group(1)) if is_apoplastic else 0.0

    if cyto_prob > 0 and apo_prob > 0:  # Dual effector
        dual_out.write(f"{header}\n{sequence}\n")
    elif cyto_prob > 0:  # Cytoplasmic effector
        cyto_out.write(f"{header}\n{sequence}\n")
    elif apo_prob > 0:  # Apoplastic effector
        apo_out.write(f"{header}\n{sequence}\n")

def split_fasta(input_file, chunk_size=750):
    output_prefix = input_file.replace(".fasta", "_chunk")
    chunk_files = []

    with open(input_file, "r") as f:
        sequences = []
        current_seq = ""
        header = ""

        for line in f:
            line = line.strip()
            if line.startswith(">"):
                if header and current_seq:
                    sequences.append((header, current_seq))
                header = line
                current_seq = ""
            else:
                current_seq += line

        if header and current_seq:
            sequences.append((header, current_seq))

        for i in range(0, len(sequences), chunk_size):
            chunk = sequences[i:i + chunk_size]
            output_file = f"{output_prefix}_part{i//chunk_size + 1}.fasta"
            chunk_files.append(output_file)
            with open(output_file, "w") as out_f:
                for h, seq in chunk:
                    out_f.write(f"{h}\n{seq}\n")

    return chunk_files

def submit_to_interpro(fasta_file):
    """
    Submit protein sequences to the InterProScan REST API.

    Parameters:
        fasta_file (str): Path to the FASTA file containing sequences.

    Returns:
        str: Job ID for the submitted job.
    """
    url = "https://www.ebi.ac.uk/Tools/services/rest/iprscan5/run/"
    with open(fasta_file, "rb") as fasta:
        # Include both email and sequence file in the POST request
        files = {"sequence": fasta}
        data = {
            "email": "abhinavrana18july@gmail.com",  # Replace with a valid email address
            "title": "InterProScan job",        # Optional title for your job
        }
        response = requests.post(url, files=files, data=data)

        # Check response status
        if response.status_code == 200:
            return response.text.strip()  # Return the job ID
        else:
            raise Exception(f"Error submitting job: {response.status_code} {response.text}")

def check_status(job_id):
    """
    Check the status of the InterProScan job.

    Parameters:
        job_id (str): The job ID.

    Returns:
        str: The job status (e.g., "RUNNING", "FINISHED", "ERROR").
    """
    url = f"https://www.ebi.ac.uk/Tools/services/rest/iprscan5/status/{job_id}"
    response = requests.get(url)
    if response.status_code == 200:
        return response.text
    else:
        raise Exception(f"Error checking status: {response.text}")

def retrieve_results(job_id):
    """
    Retrieve results for a completed InterProScan job.

    Parameters:
        job_id (str): The job ID.

    Returns:
        str: Results in TSV format.
    """
    url = f"https://www.ebi.ac.uk/Tools/services/rest/iprscan5/result/{job_id}/tsv"
    response = requests.get(url)
    if response.status_code == 200:
        return response.text
    else:
        raise Exception(f"Error retrieving results: {response.text}")

def process_results(results, fasta_file):
    """
    Process the InterProScan results to extract PFAM domains with names,
    and dynamically parse Accession and Sequence_Name.

    Parameters:
        results (str): Results in TSV format.
        fasta_file (str): Path to the input FASTA file.

    Returns:
        pd.DataFrame: DataFrame with sequence details and PFAM domain names.
    """
    rows = []
    for line in results.strip().split("\n"):
        if line.startswith("#"):  # Ignore comment lines
            continue
        parts = line.split("\t")
        sequence_name = parts[0]
        database = parts[3]
        domain_acc = parts[4]  # Domain accession number (e.g., PF00931)
        domain_name = parts[5]  # Domain name (e.g., NB-ARC)

        # Filter only PFAM domains
        if database == "Pfam":
            rows.append((sequence_name, domain_acc, domain_name))

    # Aggregate domains for each sequence
    domain_dict = {}
    for sequence_name, domain_acc, domain_name in rows:
        if sequence_name not in domain_dict:
            domain_dict[sequence_name] = []
        domain_dict[sequence_name].append((domain_acc, domain_name))

    # Convert to DataFrame
    data = []
    for record in SeqIO.parse(fasta_file, "fasta"):
        # Split the description to separate Accession and Sequence_Name
        accession = record.id  # First part is the accession (e.g., XP_042375699.1)
        sequence_name = " ".join(record.description.split(" ")[1:])  # Everything after the accession
        sequence = str(record.seq)
        domains = domain_dict.get(record.id, [])
        domain_names = [f"{domain_acc} ({domain_name})" for domain_acc, domain_name in domains]
        row = [accession, sequence_name, sequence] + domain_names
        data.append(row)

    # Create column names
    max_domains = max(len(row) - 3 for row in data)  # Find max domains
    columns = ["Accession", "Sequence_Name", "Sequence"] + [f"PFAM_Domain_{i+1}" for i in range(max_domains)]
    return pd.DataFrame(data, columns=columns)

# Main script
input_fasta_file = "result.fasta"

# Step 1: Separate the fasta file into 3 effector types
effector_files = separator(input_fasta_file)

# Step 2: Create batch files for each effector type fasta file
for file_name in effector_files:
    chunk_files = split_fasta(file_name)

    # Step 3: Submit sequences to InterPro
    df_list = []
    for chunk_file in chunk_files:
        job_id = submit_to_interpro(chunk_file)
        print(f"Job submitted for {chunk_file}. Job ID: {job_id}")

        # Step 4: Check job status
        while True:
            status = check_status(job_id)
            print(f"Job status: {status}")
            if status == "FINISHED":
                break
            elif status == "ERROR":
                raise Exception("Error occurred during InterProScan job.")
            time.sleep(30)  # Wait for 30 seconds before checking again

        # Step 5: Retrieve and process results
        results = retrieve_results(job_id)
        df = process_results(results, chunk_file)
        df_list.append(df)

    df_effector = pd.concat(df_list, ignore_index=True)

    # Step 6: Save to Excel file or display
    effector_excel_file_name = file_name.replace(".fasta", "_output.xlsx")

    df_effector.to_excel(effector_excel_file_name, index=False)
    print(f"DataFrame stored in {effector_excel_file_name}")



# # DO NOT RUN THIS CELL

# import os
# import glob

# # Delete specific batch files
# batch_files = glob.glob("*_output.xlsx")
# for file in batch_files:
#     os.remove(file)

# print("Batch files deleted successfully!")